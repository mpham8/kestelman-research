{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from transformers import pipeline as transcribe_pipeline\n",
    "import torchaudio\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "from config import HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/michael/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pyannote pipeline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86aa49eb4f947b2bf3ad9662cc02d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.yaml:   0%|          | 0.00/469 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64841d9baa994b59879574a5b78705b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164a5c726f2c4490b2a7c9aeb6ccba71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed53593278747d58e8efefa19025601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173f939d26e342d4aa614837a0d9fdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading whisper model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5fef3ad1e64570ab781a259a717124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73323b869ca4fd083534f5f162b69c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623c85b1be8d42f0b56cfbdcf1bd4000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/3.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33485203684248efb04dc82f91a4e3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61937601f9a45f698ba28c52f99414a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0471597a88847e19f716adbc2b50502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525f463711cc44989d478c24104004bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7eb677c55234c60a6dc6003852a0bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04a49c872e243e880ccfcd4c7074624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d8a910a9ec4e52a1b6106748a6ff1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1f7a0a2214489a8b860193efcd2670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing diarization\n"
     ]
    }
   ],
   "source": [
    "# Initialize pyannote pipeline for speaker diarization\n",
    "print(\"loading pyannote pipeline\")\n",
    "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\")\n",
    "\n",
    "# Initialize Whisper model for ASR (automatic speech recognition)\n",
    "print(\"loading whisper model\")\n",
    "whisper_model = transcribe_pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")\n",
    "\n",
    "# Path to your audio file\n",
    "audio_file = \"09-07-2017 audio_6 APCNV-2016-565.mp3\"\n",
    "\n",
    "# Perform diarization\n",
    "print(\"performing diarization\")\n",
    "diarization = diarization_pipeline(audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "# model_id = \"openai/whisper-large-v3\"\n",
    "model_id = \"openai/whisper-base\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading audio file\n",
      "processing segments\n",
      "Speaker 01:  Hello, everyone. My name is Lucerito Martinez. I'm the project planner assigned to the case. The project consists of the demolition of an existing single family home and the subsequent construction of a three-story eight-unit apartment building with a semi-subterranean garage. It will provide a total of 15 parking spaces. The requested entitlements include a zone change from R1-1 to R3-1 to construct the 8-unit apartment building and also include say 15-foot building line removal created by ordinance 1-29661. The recommendation is to approve the categorical exemption class 32, the zone change from R1-1 to TQR3-1.\n",
      "Speaker 09:  That's the report. Do we have any questions from the Commission for the Department on the report?\n",
      "Speaker 04:  No questions.\n",
      "Speaker 09:  Okay.\n",
      "Speaker 04:  you\n",
      "Speaker 09:  Thank you.\n",
      "Speaker 05:  you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was an error while processing timestamps, we haven't found a timestamp as last token. Was WhisperTimeStampLogitsProcessor used?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 00:  you\n",
      "Speaker 04:  I'm sending this one.\n",
      "Speaker 00:  you\n",
      "Speaker 09:  It's a general comment request.\n",
      "Speaker 05:  If there's a comment card...\n",
      "Speaker 09:  i don't have i don't have any cards for against the side\n",
      "Speaker 04:  comment on item six.\n",
      "Speaker 07:  Yeah.\n",
      "Speaker 04:  Are we?\n",
      "Speaker 07:  Yeah, this is where NACA before.\n",
      "Speaker 09:  I'm just happy.\n",
      "Speaker 03:  Good.\n",
      "Speaker 09:  By the way\n",
      "Speaker 08:  yes i'd like to know what is the name of the puppet well bully bull bully bull but the ball everybody goes by must\n",
      "Speaker 09:  But I'm going to be back.\n",
      "Speaker 08:  by their name.\n",
      "Speaker 09:  Thank you.\n",
      "Speaker 08:  Mr. Puppet. Well, no, but he's known as Mr. Puppet. Okay. Please go ahead. You got one minute.\n",
      "Speaker 09:  you\n",
      "Speaker 06:  So on my comment card, they put down general comments about item six. So on the corner on the top left, is that right?\n",
      "Speaker 08:  Web.\n",
      "Speaker 06:  There's an item number that's listed.\n",
      "Speaker 09:  Thank you.\n",
      "Speaker 03:  Bravo, Vegas, Senior Adam. And yes, it is another speaker card for item six. The address on the speaker card was actually incorrect. It should be Edawanda and I believe it reads. Nordoff, I asked the individual to fill out the speaker card so he can submit it, which I would then hand over to the Commission President so that the individual can have a minute to speak on the item.\n",
      "Speaker 09:  Okay. So is he going to speak? Do you have a comment card or not? Yes, he will speak. He's currently filling it out. All right. We'll give him a minute or two to do that.\n",
      "Speaker 02:  Is that item number six which you'll look at?\n",
      "Speaker 09:  We're in number six right now.\n",
      "Speaker 02:  All right.\n",
      "Speaker 09:  Fine.\n",
      "Speaker 05:  Not.\n",
      "Speaker 09:  No, no, I don't. Okay. At this point, the chair will entertain a motion from a commissioner.\n",
      "Speaker 05:  No, I don't.\n",
      "Speaker 04:  On this item I'm going to make a motion please. That's for case number APC envy 2016 565 ZCBL and sequel number ENV 2016 564 CE this is Commissioner Hartwinian so\n",
      "Speaker 09:  Okay.\n",
      "Speaker 04:  Okay, we have a motion.\n",
      "Speaker 09:  on the floor, can we get a second?\n",
      "Speaker 07:  And second, the motion, this is Commissioner Garcia.\n",
      "Speaker 09:  Okay, we have a motion on the floor, a second call for the role.\n",
      "Speaker 07:  Just Renee C and you have a motion to approve the staff\n",
      "Speaker 04:  you\n",
      "Speaker 07:  Commissioner Garcia, how do you vote? Aye. And Commissioner.\n",
      "Speaker 09:  I'm sorry.\n",
      "Speaker 10:  you\n",
      "Speaker 07:  This is where an ACA we're being recorded and nothing is made. I want to speak. Come up with a microphone on your phone.\n",
      "Speaker 09:  I want to speak, come up with a microphone on your M. Please identify yourself.\n",
      "Speaker 10:  Robert Lamishaw, JPL zoning on behalf of the applicant. I just wonder if you had a projected date for the plum hearing on this.\n",
      "Speaker 09:  Thank you.\n",
      "Speaker 10:  you\n",
      "Speaker 09:  I don't believe the board does. Okay. Check with the planning department.\n",
      "Speaker 00:  This Christine Sapnara from Planning Department usually once a project is recommended for approval. It's up to the agenda of CD-14 to place it on the plum agenda. So they do their own well thing, they'll let you know though.\n",
      "Speaker 10:  Okay.\n",
      "Speaker 09:  Okay, we have nothing else on the agenda. I'm going to take an emotion for adjournment.\n",
      "Speaker 04:  Second.\n",
      "Speaker 07:  Okay, and we are adjourned and we're out. Please. Give me a time. A time.\n",
      "Speaker 09:  We're out.\n",
      "Speaker 07:  He can come to the end.\n",
      "Speaker 09:  452.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the audio file\n",
    "print(\"loading audio file\")\n",
    "# waveform, sample_rate = torchaudio.load(audio_file)\n",
    "\n",
    "# audio = AudioSegment.from_mp3(audio_file)\n",
    "waveform, sample_rate = librosa.load(audio_file, sr=16000)  # Load and resample to 16kHz\n",
    "\n",
    "\n",
    "\n",
    "# Process each segment and transcribe\n",
    "transcriptions = []\n",
    "print(\"processing segments\")\n",
    "\n",
    "previous_speaker = None\n",
    "current_speaker_text = \"\"\n",
    "speaker_ls = []\n",
    "speaker_text_ls = []\n",
    "for segment in diarization.itertracks(yield_label=True):\n",
    "    start_time = segment[0].start\n",
    "    end_time = segment[0].end\n",
    "    speaker_label = segment[2]\n",
    "    \n",
    "    # Extract the segment from the waveform\n",
    "    # start_sample = int(start_time * sample_rate)\n",
    "    # end_sample = int(end_time * sample_rate)\n",
    "    # segment_waveform = waveform[:, start_sample:end_sample]\n",
    "    # segment_waveform = waveform[start_time:end_time]\n",
    "        # Convert time to sample indices\n",
    "    start_sample = int(start_time * sample_rate)\n",
    "    end_sample = int(end_time * sample_rate)\n",
    "    \n",
    "    # Ensure indices are within bounds\n",
    "    start_sample = max(0, start_sample)\n",
    "    end_sample = min(len(waveform), end_sample)\n",
    "    \n",
    "    # Extract the segment from the waveform\n",
    "    segment_waveform = waveform[start_sample:end_sample]\n",
    "\n",
    "\n",
    "    transcription = pipe(segment_waveform)\n",
    "    # transcription = result[\"text\"]\n",
    "\n",
    "    # Convert the segment to text using Whisper\n",
    "    # transcription = whisper_model(segment_waveform.squeeze().numpy(), return_timestamps=False)\n",
    "    current_speaker = speaker_label.split('_')[-1]\n",
    "    \n",
    "    # Handle the first speaker\n",
    "    if previous_speaker is None:\n",
    "        current_speaker_text = transcription['text']\n",
    "\n",
    "\n",
    "    elif current_speaker != previous_speaker:\n",
    "        # Append the previous speaker's text if it exists\n",
    "        if current_speaker_text:\n",
    "            speaker_ls.append(int(previous_speaker.split('_')[-1]))\n",
    "            speaker_text_ls.append(current_speaker_text)\n",
    "            # transcriptions.append(f\"Speaker {previous_speaker.split('_')[-1]}: {current_speaker_text}\")\n",
    "        current_speaker_text = transcription['text']\n",
    "        print(f\"Speaker {speaker_label.split('_')[-1]}: {transcription['text']}\")\n",
    "    else:\n",
    "        current_speaker_text += \" \" + transcription['text']\n",
    "\n",
    "    previous_speaker = current_speaker\n",
    "\n",
    "    # Handle the last speaker after the loop\n",
    "if current_speaker_text:\n",
    "    # transcriptions.append(f\"Speaker {current_speaker}: {current_speaker_text}\")\n",
    "    speaker_ls.append(int(current_speaker.split('_')[-1]))\n",
    "    speaker_text_ls.append(current_speaker_text)\n",
    "\n",
    "# # Print the transcriptions\n",
    "# for transcription in transcriptions:\n",
    "#     print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Next item on the agenda is some  It's the cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hello, everyone. My name is Lucerito Martinez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>That's the report. Do we have any questions f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No questions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Okay.  Hearing none and there's no other spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4</td>\n",
       "      <td>Second.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>7</td>\n",
       "      <td>Okay, and we are adjourned and we're out. Ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>9</td>\n",
       "      <td>We're out.  the way the main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>7</td>\n",
       "      <td>He can come to the end.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>9</td>\n",
       "      <td>452.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    speaker                                               text\n",
       "0         9   Next item on the agenda is some  It's the cas...\n",
       "1         1   Hello, everyone. My name is Lucerito Martinez...\n",
       "2         9   That's the report. Do we have any questions f...\n",
       "3         4                                      No questions.\n",
       "4         9   Okay.  Hearing none and there's no other spea...\n",
       "..      ...                                                ...\n",
       "60        4                                            Second.\n",
       "61        7   Okay, and we are adjourned and we're out. Ple...\n",
       "62        9                       We're out.  the way the main\n",
       "63        7                            He can come to the end.\n",
       "64        9                                               452.\n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"speaker\": speaker_ls,\n",
    "    \"text\": speaker_text_ls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(audio_file.replace('.mp3', '') + \".csv\", index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
