{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "from login import EMAIL\n",
    "from login import PASSWORD\n",
    "from downloaded import DOWNLOADED_SET\n",
    "import re\n",
    "import requests\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login\n",
    "def boot_and_login():\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    driver.get(\"https://pro.urbanize.city/users/sign_in\")\n",
    "\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "\n",
    "    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "    if iframes:\n",
    "        driver.switch_to.frame(iframes[0])\n",
    "\n",
    "    email_input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"input[name='email'], input[type='email']\")))\n",
    "    email_input.send_keys(EMAIL)\n",
    "\n",
    "    password_input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"input[name='password'], input[type='password']\")))\n",
    "    password_input.send_keys(PASSWORD)\n",
    "\n",
    "    login_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.btn.prime[actionlogin]\")))\n",
    "    login_button.click()\n",
    "\n",
    "\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "    driver.switch_to.default_content()\n",
    "    wait.until(EC.visibility_of_element_located((By.XPATH, \"//a[contains(@class, 'border-cyan-500') and contains(text(), 'Dashboard')]\")))\n",
    "\n",
    "\n",
    "    return driver, wait\n",
    "\n",
    "driver, wait = boot_and_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_info(driver, wait, url):\n",
    "  driver.get(url)\n",
    "\n",
    "  #get project name\n",
    "  wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "  project_name = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.text-5xl.font-semibold.text-gray-900.tracking-tight.mb-2\"))).text\n",
    "\n",
    "  print(f\"Project Name: {project_name}\")\n",
    "  \n",
    "\n",
    "  #get project status\n",
    "  status = None\n",
    "  status_elements = driver.find_elements(By.CSS_SELECTOR, \"div.flex.space-x-1.w-full > div\")\n",
    "  for element in status_elements:\n",
    "    bg_div = element.find_element(By.CSS_SELECTOR, \"div[class*='w-full'][class*='h-2.5']\")\n",
    "    if \"bg-cyan-600\" in bg_div.get_attribute(\"class\"):\n",
    "      status = element.find_element(By.CSS_SELECTOR, \"div.text-sm\").text\n",
    "\n",
    "  print(f\"Project Status: {status}\")\n",
    "\n",
    "\n",
    "  #get location\n",
    "  location_header = driver.find_element(By.XPATH, \"//h3[contains(@class, 'text-xl') and contains(text(), 'Location')]\")\n",
    "  location_html = location_header.find_element(By.XPATH, \"following-sibling::p\").get_attribute('innerHTML')\n",
    "  # location_text = location_element2.text.replace('\\n', ', ')\n",
    "  location_text = re.sub('<[^<]+?>', '', location_html).replace('\\n', '').strip()\n",
    "  # location_text = ', '.join(part.strip() for part in location_text.split(','))\n",
    "\n",
    "  print(f\"Project Location: {location_text}\")\n",
    "\n",
    "\n",
    "  companies = {}\n",
    "  try:\n",
    "      companies_div = driver.find_element(By.XPATH, \"//h3[contains(@class, 'text-xl') and contains(text(), 'Companies')]/ancestor::div[contains(@class, 'project-companies')]\")\n",
    "      company_items = companies_div.find_elements(By.XPATH, \".//li[contains(@class, 'py-2')]\")\n",
    "      \n",
    "      for item in company_items:\n",
    "          label = item.find_element(By.XPATH, \".//div[contains(@class, 'text-slate-900') and contains(@class, 'font-medium')]\").text\n",
    "          value_div = item.find_element(By.XPATH, \".//div[contains(@class, 'space-y-1')]\")\n",
    "          company_elements = value_div.find_elements(By.CSS_SELECTOR, \"div.text-cyan-600.font-semibold a\")\n",
    "          companies[label] = [element.text for element in company_elements]\n",
    "          print(f\"{label}: {', '.join(companies[label])}\")\n",
    "  except:\n",
    "      print(\"Companies section not found or could not be parsed\")\n",
    "\n",
    "  project_info = {}\n",
    "  try:\n",
    "        information_div = driver.find_element(By.XPATH, \"//h3[contains(@class, 'text-xl') and contains(text(), 'Information')]/ancestor::div[contains(@class, '')]\")\n",
    "        info_items = information_div.find_elements(By.XPATH, \".//li[contains(@class, 'py-4')]\")\n",
    "        \n",
    "        for item in info_items:\n",
    "            label = item.find_element(By.XPATH, \".//div[contains(@class, 'text-slate-900') and contains(@class, 'font-medium')]\").text\n",
    "            value_div = item.find_element(By.XPATH, \".//div[contains(@class, 'text-slate-800') and contains(@class, 'font-normal')]\")\n",
    "            \n",
    "            if label == \"Project uses\":\n",
    "                uses = [use.text for use in value_div.find_elements(By.XPATH, \".//div/div\")]\n",
    "                project_info[label] = uses\n",
    "                print(f\"Project uses: {', '.join(uses)}\")\n",
    "            else:\n",
    "                value = value_div.text.strip()\n",
    "                project_info[label] = value\n",
    "                print(f\"{label}: {value}\")\n",
    "  except:\n",
    "        print(\"Information section not found or could not be parsed\")\n",
    "\n",
    "    # Return the extracted data as a dictionary\n",
    "  result = {\n",
    "        \"project_name\": project_name,\n",
    "        \"status\": status,\n",
    "        \"location\": location_text,\n",
    "        **companies,\n",
    "        **project_info\n",
    "    }\n",
    "    \n",
    "  print(result)\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pull_data('https://pro.urbanize.city/los_angeles/projects/d506b9d2/2143-e-violet-street')\n",
    "# pull_data('https://pro.urbanize.city/los_angeles/projects/7018f2cb/st-ambrose-senior-housing')\n",
    "# pull_data('https://pro.urbanize.city/los_angeles/projects/4d6f451e/huntington-hospital-758-fair-oaks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_folder = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(driver, wait, url, article_folder):\n",
    "  driver.get(url)\n",
    "\n",
    "  wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"article-lead-image\")))\n",
    "\n",
    "  #get lead image\n",
    "  lead_image_div = driver.find_element(By.CLASS_NAME, \"article-lead-image\")\n",
    "  lead_image = lead_image_div.find_element(By.TAG_NAME, \"img\")\n",
    "  lead_image_url = lead_image.get_attribute('src')\n",
    "\n",
    "  image_urls = [lead_image_url] if lead_image_url else []\n",
    "\n",
    "  \n",
    "  wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "  #get other images\n",
    "  images = driver.find_elements(By.XPATH, \"//img[@class='image-950w article-inline-image']\")\n",
    "  \n",
    "  for image in images:\n",
    "      src = image.get_attribute('src')\n",
    "      if src:\n",
    "          image_urls.append(src)\n",
    "  \n",
    "\n",
    "  #get h1 class = \"article-title\"\n",
    "  article_title = driver.find_element(By.CSS_SELECTOR, \"h1.article-title\").text\n",
    "\n",
    "  #get h2 class = \" article-subtitle\"\n",
    "  article_subtitle = driver.find_element(By.CSS_SELECTOR, \"h2.article-subtitle\").text\n",
    "\n",
    "  #get p in class=\"article-body\"\n",
    "  article_body_div = driver.find_element(By.CSS_SELECTOR, \"div.article-body\")\n",
    "  paragraphs = article_body_div.find_elements(By.CSS_SELECTOR, \"p:not(.image-and-caption)\")\n",
    "  article_body = \"\\n\".join([p.text for p in paragraphs])\n",
    "\n",
    "\n",
    "  #get comment    \n",
    "  wait.until(EC.presence_of_element_located((By.TAG_NAME, \"iframe\")))\n",
    "    \n",
    "  #switch to  iframe\n",
    "  iframe = driver.find_element(By.TAG_NAME, \"iframe\")\n",
    "  driver.switch_to.frame(iframe)\n",
    "    \n",
    "  wait.until(EC.presence_of_element_located((By.ID, \"post-list\")))\n",
    "  comments = driver.find_elements(By.CSS_SELECTOR, \"li.post\")\n",
    "    \n",
    "  comment_data = []\n",
    "    \n",
    "  for comment in comments:\n",
    "        comment_id = comment.get_attribute(\"id\")\n",
    "        author = comment.find_element(By.CSS_SELECTOR, \"span.author\").text\n",
    "        content = comment.find_element(By.CSS_SELECTOR, \"div.post-message\").text\n",
    "        timestamp = comment.find_element(By.CSS_SELECTOR, \"a.time-ago\").get_attribute(\"title\")\n",
    "        \n",
    "        parent_link = comment.find_elements(By.CSS_SELECTOR, \"a.parent-link\")\n",
    "        parent_id = parent_link[0].get_attribute(\"href\").split(\"#\")[-1] if parent_link else None\n",
    "        \n",
    "        # Store comment data\n",
    "        comment_data.append({\n",
    "            \"id\": comment_id,\n",
    "            \"author\": author,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"parent_id\": parent_id\n",
    "        })\n",
    "    \n",
    "    # driver.switch_to.default_content()\n",
    "\n",
    "  article_image_folder = os.path.join(article_folder, \"images\")\n",
    "  os.makedirs(article_image_folder, exist_ok=True)\n",
    "\n",
    "  print(\"Article Title:\", article_title)\n",
    "  print(\"Article Subtitle:\", article_subtitle)\n",
    "  print(\"Article Body:\", article_body)\n",
    "  for i, image_url in enumerate(image_urls, 1):\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "      print(f\"Image {i}: {image_url}\")\n",
    "\n",
    "      image_filename = f\"image_{i}.jpg\"\n",
    "      image_path = os.path.join(article_image_folder, image_filename)\n",
    "      with open(image_path, 'wb') as f:\n",
    "          f.write(response.content)\n",
    "      # print(f\"Image {i} saved as: {image_path}\")\n",
    "      # display(Image(response.content))\n",
    "  print(\"Comments:\", comment_data)\n",
    "    \n",
    "  return article_title, article_subtitle, article_body, image_urls, comment_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# article_title, article_subtitle, article_body, image_urls, comment_data = pull_article_data('https://la.urbanize.city/post/updated-look-new-huntington-hospital-medical-offices')\n",
    "\n",
    "\n",
    "# print(\"Article Title:\", article_title)\n",
    "# print(\"Article Subtitle:\", article_subtitle)\n",
    "# print(\"Article Body:\", article_body)\n",
    "# for i, url in enumerate(image_urls, 1):\n",
    "#       print(f\"Image {i}: {url}\")\n",
    "  \n",
    "\n",
    "# for image_url in image_urls:\n",
    "#     response = requests.get(image_url)\n",
    "#     if response.status_code == 200:\n",
    "#         display(Image(response.content))\n",
    "\n",
    "# print(\"Comments:\", comment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "PROJECT NAME: St Ambrose Senior Housing\n",
      "PROJECT ID: 7018f2cb_st-ambrose-senior-housing\n",
      "*********************\n",
      "Project Name: St Ambrose Senior Housing\n",
      "Project Status: Proposed\n",
      "Project Location: 830 Bonita Avenue    Glendora, CA 91740\n",
      "Developer: St. Ambrose Episcopal Church\n",
      "Project uses: Apartments\n",
      "Podium: No\n",
      "# of apartments: 59\n",
      "Construction type: Type 3 4 5\n",
      "{'project_name': 'St Ambrose Senior Housing', 'status': 'Proposed', 'location': '830 Bonita Avenue    Glendora, CA 91740', 'Developer': ['St. Ambrose Episcopal Church'], 'Project uses': ['Apartments'], 'Podium': 'No', '# of apartments': '59', 'Construction type': 'Type 3 4 5'}\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\nRemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\nWebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:193:5\nNoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:511:5\ndom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#for each url get the project info\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m page_urls:\n\u001b[1;32m    120\u001b[0m    \u001b[38;5;66;03m#TO DO: check if project is already downloaded\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m    \u001b[43mget_project_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 62\u001b[0m, in \u001b[0;36mget_project_data\u001b[0;34m(driver, wait, url)\u001b[0m\n\u001b[1;32m     59\u001b[0m article_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_save_folder, project_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m\"\u001b[39m, article_id)\n\u001b[1;32m     60\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(article_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 62\u001b[0m article_title, article_subtitle, article_body, image_urls, comment_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_article_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m article_csv_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(article_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticleinfo.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m article_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: article_title,\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: article_subtitle,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m: article_body\n\u001b[1;32m     69\u001b[0m }\n",
      "Cell \u001b[0;32mIn[52], line 44\u001b[0m, in \u001b[0;36mget_article_info\u001b[0;34m(driver, wait, url, article_folder)\u001b[0m\n\u001b[1;32m     41\u001b[0m iframe \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mTAG_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miframe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m driver\u001b[38;5;241m.\u001b[39mswitch_to\u001b[38;5;241m.\u001b[39mframe(iframe)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost-list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m comments \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mli.post\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m comment_data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\nRemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\nWebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:193:5\nNoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:511:5\ndom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n"
     ]
    }
   ],
   "source": [
    "def get_page_project_urls(driver):\n",
    "  project_links = driver.find_elements(By.CSS_SELECTOR, \"ul.grid a[href^='/los_angeles/projects/']\")\n",
    "  page_urls = [link.get_attribute('href') for link in project_links]\n",
    "\n",
    "  return page_urls\n",
    "\n",
    "def get_project_data(driver, wait, url):\n",
    "  \n",
    "\n",
    "  driver.get(url)\n",
    "\n",
    "  # Extract the last two parts of the URL\n",
    "  url_parts = url.rstrip('/').split('/')\n",
    "  last_two_parts = '_'.join(url_parts[-2:])\n",
    "  project_id = last_two_parts.replace('/', '_')\n",
    "  \n",
    "  # print(f\"Extracted project ID: {project_id}\")\n",
    "\n",
    "  wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    \n",
    "  project_name = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.text-5xl.font-semibold.text-gray-900.tracking-tight.mb-2\"))).text\n",
    "\n",
    "  # project_data[url]['project_name'] = project_name\n",
    "  print(\"*\"*21)\n",
    "  print(\"PROJECT NAME:\", project_name)\n",
    "  print(\"PROJECT ID:\", project_id)\n",
    "  print(\"*\"*21)\n",
    "\n",
    "  project_info_dict = get_project_info(driver, wait, url)\n",
    "\n",
    "  # Export project info to CSV\n",
    "  project_folder = os.path.join(data_save_folder, project_id)\n",
    "  os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "  csv_filename = os.path.join(data_save_folder, project_id, f\"projectinfo.csv\")\n",
    "  with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "      writer = csv.DictWriter(csvfile, fieldnames=project_info_dict.keys())\n",
    "      writer.writeheader()\n",
    "      writer.writerow(project_info_dict)\n",
    "  # print(f\"Project info exported to {csv_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "  articles_folder = os.path.join(data_save_folder, project_id, \"articles\")\n",
    "  os.makedirs(articles_folder, exist_ok=True)\n",
    "\n",
    "  #get article urls for each project\n",
    "  grid_exists = len(driver.find_elements(By.CSS_SELECTOR, \"ul.grid\")) > 0\n",
    "\n",
    "  if grid_exists:\n",
    "      grid_links = driver.find_elements(By.CSS_SELECTOR, \"ul.grid li a\")\n",
    "      grid_urls = [link.get_attribute('href') for link in grid_links]\n",
    "      project_data[url]['grid_urls'] = grid_urls\n",
    "      for article_url in grid_urls:\n",
    "         #get article data(article url)\n",
    "\n",
    "        # Extract the last part of the article URL\n",
    "        article_id = article_url.rstrip('/').split('/')[-1]\n",
    "        article_folder = os.path.join(data_save_folder, project_id, \"articles\", article_id)\n",
    "        os.makedirs(article_folder, exist_ok=True)\n",
    "        \n",
    "        article_title, article_subtitle, article_body, image_urls, comment_data = get_article_info(driver, wait, article_url, article_folder)\n",
    "\n",
    "        article_csv_filename = os.path.join(article_folder, f\"articleinfo.csv\")\n",
    "        article_info = {\n",
    "            'title': article_title,\n",
    "            'subtitle': article_subtitle,\n",
    "            'body': article_body\n",
    "        }\n",
    "        with open(article_csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=article_info.keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerow(article_info)\n",
    "\n",
    "        # Export comment data to CSV\n",
    "        comment_csv_filename = os.path.join(article_folder, f\"comments.csv\")\n",
    "        if comment_data:\n",
    "            with open(comment_csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=comment_data[0].keys())\n",
    "                writer.writeheader()\n",
    "                for comment in comment_data:\n",
    "                    writer.writerow(comment)\n",
    "\n",
    "        # print(\"Article Title:\", article_title)\n",
    "        # print(\"Article Subtitle:\", article_subtitle)\n",
    "        # print(\"Article Body:\", article_body)\n",
    "        # for i, url in enumerate(image_urls, 1):\n",
    "        #       print(f\"Image {i}: {url}\")\n",
    "          \n",
    "\n",
    "        # for image_url in image_urls:\n",
    "        #     response = requests.get(image_url)\n",
    "        #     if response.status_code == 200:\n",
    "        #         display(Image(response.content))\n",
    "\n",
    "        # print(\"Comments:\", comment_data)\n",
    "\n",
    "\n",
    "          # driver.get(article_url)\n",
    "          # wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "  #pull data for each project\n",
    "\n",
    "\n",
    "# driver, wait = boot_and_login()\n",
    "page_number = 1\n",
    "seen_all = False\n",
    "\n",
    "while seen_all == False:\n",
    "  #go to next page\n",
    "  page_number += 1\n",
    "  driver.get(f\"https://pro.urbanize.city/los_angeles/projects?page={page_number}\")\n",
    "\n",
    "  #get project url from each page\n",
    "  page_urls = get_page_project_urls(driver)\n",
    "  project_data = {url: {} for url in page_urls}\n",
    "\n",
    "  #for each url get the project info\n",
    "  for url in page_urls:\n",
    "     #TO DO: check if project is already downloaded\n",
    "     get_project_data(driver, wait, url)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
